Q1: What is Machine Learning?

Answer:
Machine Learning (ML) is a branch of AI that enables computers to learn patterns from data and make predictions or decisions without being explicitly programmed for the task.

Example: Spam email detection, house price prediction.

Key idea: Instead of writing rules, we give data and let the system learn patterns.


Q2: What is a Machine Learning Model?

Answer:
A model is the mathematical representation of learned patterns from data. It can take inputs and produce predictions.

Represented as ùëì(ùë•;ùúÉ) where:
    x = input features
    Œ∏ = parameters learned during training
    f(x;Œ∏) = output prediction
Example: Linear regression model ‚Üí y=wx+b, where w and b are learned.

Q3: Difference between Algorithm and Model?

Answer:
| Term      | Explanation                                                                           |
| --------- | ------------------------------------------------------------------------------------- |
| Algorithm | Step-by-step procedure used to train a model (e.g., Linear Regression, Random Forest) |
| Model     | The trained version of the algorithm with learned parameters                          |

Example:
Algorithm: Random Forest
Model: Trained forest that predicts new data


Q4: Types of Machine Learning

Answer:

Supervised Learning -> Learns from labeled data (input-output pairs)

Example: Predict house prices, email spam detection

Unsupervised Learning -> Learns patterns from unlabeled data

Example: Customer segmentation, anomaly detection

Reinforcement Learning -> Learns by interacting with an environment and receiving rewards

Example: Game AI, self-driving cars


Q5: Examples of Supervised vs Unsupervised

Answer:
| Type          | Example                                    |
| ------------- | ------------------------------------------ |
| Supervised    | Predict student grades from hours studied  |
| Unsupervised  | Group customers based on shopping behavior |
| Reinforcement | Training a robot to walk using rewards     |


Q6: Steps in Machine Learning Workflow

Answer:
Data Collection ‚Äì Gather raw data
Data Preprocessing ‚Äì Clean, handle missing values, encode, scale
Feature Engineering ‚Äì Create meaningful input features
Train-Test Split ‚Äì Separate data to train and evaluate the model
Model Selection & Training ‚Äì Choose algorithm and train
Evaluation ‚Äì Use metrics to check performance
Hyperparameter Tuning ‚Äì Optimize model parameters
Deployment & Monitoring ‚Äì Put model in production and track performance


Q7: What is Data Preprocessing? Why is it important?

Answer:
Data Preprocessing is the process of cleaning, transforming, and organizing raw data before feeding it into a machine learning model.

Importance:
Real-world data is often noisy, incomplete, or inconsistent.
Preprocessing improves model accuracy and convergence.
Prevents problems like overfitting, bias, or poor generalization.

Key Steps:
Handling missing values
Encoding categorical variables
Scaling / normalization
Outlier detection & handling
Feature selection & feature engineering


Q8: How do you handle missing data?

Answer:

Methods:
Delete rows ‚Äì if missing values are few
Mean / Median / Mode Imputation ‚Äì numeric or categorical
KNN Imputation ‚Äì predict missing values using nearest neighbors
Regression-based Imputation ‚Äì predict missing values from other features
‚ÄúMissing‚Äù category ‚Äì for categorical variables with many missing values

Example:
Age column has missing values ‚Üí replace with median age


Q9: How do you handle categorical data in ML?

Answer:
Techniques:

Label Encoding ‚Äì converts categories into integers (ordinal)

One-Hot Encoding ‚Äì creates binary columns (nominal)

Target Encoding ‚Äì replaces category with mean of target variable

Binary Encoding / Embeddings ‚Äì reduces dimensions for high-cardinality features

Q10: How do you handle categorical data in ML?

Answer:

Techniques:
Label Encoding ‚Äì converts categories into integers (ordinal)
One-Hot Encoding ‚Äì creates binary columns (nominal)
Target Encoding ‚Äì replaces category with mean of target variable
Binary Encoding / Embeddings ‚Äì reduces dimensions for high-cardinality features


Q11: Why do we scale or normalize features?

Answer:
Scaling ensures features are on the same scale, which is important for distance-based algorithms (KNN, K-means, SVM) and 
gradient-based optimization (Neural Networks).

Methods:
Min-Max Scaling ‚Üí scales to [0,1]
Standardization ‚Üí mean = 0, std = 1
Robust Scaling ‚Üí median and IQR, robust to outliers


Q12: What is an outlier? How do you handle them?

Answer:
Outlier: Data points significantly different from other observations.

Detection Methods:
Z-score (abs(z) > 3)
IQR method (Q1 - 1.5IQR, Q3 + 1.5IQR)
Boxplot visualization
Isolation Forest or DBSCAN

Handling Outliers:
Remove
Cap / Winsorize
Log transformation
Use robust scaling


Q13: What is multicollinearity and how to detect it?

Answer:
Multicollinearity: When independent features are highly correlated.

Detection:
Correlation matrix (heatmap)
Variance Inflation Factor (VIF > 5 ‚Üí high correlation)

Fix:
Remove correlated features
Combine features
Apply PCA
Use regularization (L2 reduces multicollinearity)


Q14: What is train-test split and why is it important?

Answer:
Split dataset into train and test sets.
Train set ‚Üí model learns patterns
Test set ‚Üí evaluate performance on unseen data

Common Split: 70% train, 30% test or 80-20.
Importance: Prevents overfitting and ensures generalization.

Q15: What is cross-validation?

Answer:
Technique to assess model performance reliably.
Common types: k-fold CV, Stratified k-fold CV
Helps in hyperparameter tuning and reduces variance compared to a single train-test split

Example:
5-fold CV ‚Üí dataset divided into 5 parts, each part is used once as test set

Q16.what is Feature Engineering Workflow
Answer:
    Feature engineering involves creating, transforming, and selecting features to improve model learning. I usually start by understanding domain relationships, creating interaction features, handling skewness, encoding categorical variables, scaling numerical features, and finally selecting the most informative features using correlation, mutual information, or regularization.

steps:
   Raw Data
      ‚Üì
   Data Cleaning
      ‚Üì
   Feature Creation
      ‚Üì
   Feature Transformation(Scaling)
      ‚Üì
   Feature Selection

Types:
    1.Feature Creation
    2.Feature Transformation
    3.Encoding Categorical Features
    4.Feature Selection
    5.Handling Multicollinearity (Feature Engineering Perspective)
    

Note:
ML Pipeline:
   Raw Data
      ‚Üì
   Preprocessing (clean, encode, scale)
      ‚Üì
   Feature Engineering (create, combine, select)
      ‚Üì
   Feature Selection (select a subset of features)
      ‚Üì
   Model Training

Q17.Can preprocessing leak information?
    Yes, preprocessing can cause data leakage if transformations like scaling, imputation, or feature selection are fitted on the entire dataset before train-test split. To prevent this, preprocessing steps must be fitted only on training data or implemented using pipelines.

    Preprocessing on the entire dataset before train‚Äìtest split.This allows the model to indirectly ‚Äúsee‚Äù test data statistics.
    1.Leakage via Scaling
    2.Leakage via Missing Value Imputation
    3.Leakage via Feature Selection
    4.Leakage via Encoding

Q18.Difference between feature engineering and feature selection?

Answer:
| Aspect                  | Feature Engineering                                                              | Feature Selection                                                                                      |
| ----------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------
| **Definition**          | Creating new features or transforming existing ones to improve model performance | Selecting the most relevant subset of features
                                                                                                                    from the existing dataset 

| **Goal**                | Add more information to help the model learn                                     | Reduce dimensionality and remove 
                                                                                                                irrelevant/redundant features.  

| **When done**           | Usually **after preprocessing**, before modeling                                 | Can be **after feature engineering** or 
                                                                                                                        preprocessing 

| **Domain Knowledge**    | High ‚Äî often needs business/domain insight                                       | Medium ‚Äî can be statistical, model-based, or 
                                                                                                                automatic   

| **Techniques**          | Interaction terms, ratios, polynomial features, binning, aggregation, encoding   | Correlation threshold, VIF, mutual information, 
                                                                                                                Recursive Feature Elimination (RFE), L1 
                                                                                                                regularization 

| **Effect on model**     | Improves learning by giving more meaningful features                             | Improves generalization, reduces overfitting, 
                                                                                                                reduces training time

| **Creativity Required** | High                                                                             | Moderate                                         



